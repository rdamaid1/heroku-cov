{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUMBmmeW2VZa"
      },
      "source": [
        "# 1. Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Segy__Q9T5_K"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPg4QdSJuzF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c0c469-7fe5-49e4-a9bf-8bf01a217412"
      },
      "source": [
        "#Connect to Google Drive\n",
        "from google.colab import drive \n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90I1Z0Wdu2FO"
      },
      "source": [
        "#Ignore warnings\n",
        "%%capture\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#Essentials\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import itertools\n",
        "from collections import Counter\n",
        "pd.options.display.max_columns = 200\n",
        "pd.options.display.max_rows = 200\n",
        "\n",
        "#Preprocessing and Modelling\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, LabelBinarizer, StandardScaler\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support, roc_curve\n",
        "from sklearn.model_selection import StratifiedKFold,KFold\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "#Deep learning\n",
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow-determinism\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Activation, AlphaDropout, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.backend import sigmoid\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad, SGD, RMSprop, Adadelta\n",
        "\n",
        "#Setting seeds and random states for reproducibility\n",
        "SEED = 0\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "# tf.keras.backend.clear_session()\n",
        "# tf.compat.v1.reset_default_graph()\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "#Checking GPU availability\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device usage is not active')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "#Swish activation function\n",
        "def swish(x, beta = 1):\n",
        "    return (x * sigmoid(beta * x))\n",
        "get_custom_objects().update({'swish': swish})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg0RP445T2SC"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KwsFnyT0Ks"
      },
      "source": [
        "#Function for evaluation\n",
        "def evaluate_dnn(model,testX,testY):\n",
        "  # Metrics and classification reports\n",
        "  print(\"[INFO] Model Performance {}\", model)\n",
        "  test_loss, test_acc = model.evaluate(testX, testY)\n",
        "  predictions = model.predict(testX)\n",
        "  roc_auc = roc_auc_score(testY, predictions)\n",
        "  print()\n",
        "  print(\"[INFO] Classification Report\")\n",
        "  print(\"Test Loss : {0:.3f} \\t Test Accuracy : {1:.3f}\".format(test_loss, test_acc))\n",
        "  print(\"ROC AUC   : {:.3f}\".format(roc_auc))\n",
        "  print(classification_report(testY,[1 if i >=0.5 else 0 for i in predictions], target_names = [\"0\",\"1\"]))\n",
        "  print()\n",
        "\n",
        "  #Confusion matrix\n",
        "  print(\"[INFO] Confusion Matrix\")\n",
        "  LABELS = [\"Negative\", \"Positive\"]\n",
        "  conf_matrix = confusion_matrix(testY, [1 if i >=0.5 else 0 for i in predictions])\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  sns.set(font_scale=1.4)\n",
        "  sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel('True class')\n",
        "  plt.xlabel('Predicted class')\n",
        "  plt.show()\n",
        "\n",
        "def sae_model(xt, xv = None, EPOCHS = 50, BATCH_SIZE = 32, opt = \"adam\", fr_node = 0.5,\n",
        "              hl_node = 1024, lr = 0.01,af = \"relu\",num_layers = 3, do = 0.1,\n",
        "              verbose = 0,return_fe = False):\n",
        "  #Setting result placeholders\n",
        "  xt_ae = [] ;xv_ae = [] ; w_ae = []\n",
        "  #If validation set is not present, use train set as validation set\n",
        "  if xv is None :\n",
        "    xv = xt.copy()\n",
        "  opt = tf.keras.optimizers.get(opt) #Set optimizer\n",
        "  K.set_value(opt.learning_rate, lr) #Set learning rate\n",
        "\n",
        "  #Stacked Autoencoder architecture\n",
        "  for n_layers in range(num_layers):\n",
        "    #Autoencoder\n",
        "    inp = Input(shape=(xt.shape[1],))\n",
        "    hidden_layer = Dropout(0.1)(inp)\n",
        "    enc = Dense(int(hl_node*(fr_node**n_layers)), activation = af)(hidden_layer)  \n",
        "    dec = Dense(xt.shape[1],activation=\"linear\")(enc)\n",
        "    ae = Model(inp, dec)\n",
        "\n",
        "    ae.compile(optimizer=opt, loss='mean_squared_error')\n",
        "    es = EarlyStopping(monitor='val_loss', patience=15, verbose=verbose)\n",
        "    ae.fit(xt, xt, \n",
        "           epochs=EPOCHS,batch_size=BATCH_SIZE, \n",
        "           shuffle=True, callbacks = [es] , verbose = verbose,\n",
        "           validation_data = (xv,xv))\n",
        "\n",
        "    fe = Model(ae.input, enc)\n",
        "    xt = fe.predict(xt) ; xt_ae.append(xt)\n",
        "    xv = fe.predict(xv) ; xv_ae.append(xv)\n",
        "    w_ae.append([layer_name for layer_name in ae.layers if \"dense\" in layer_name.name][0].get_weights())\n",
        "    if verbose:\n",
        "      print(\"Layer {} trained\".format(n_layers+1))\n",
        "\n",
        "  return (w_ae,xv) if return_fe else w_ae\n",
        "\n",
        "\n",
        "def dnn_model(xt, sae_weights = None, EPOCHS = 50,BATCH_SIZE = 32, opt = \"adam\",\n",
        "              hl_node = 1024, lr = 0.01,af = \"relu\",num_layers = 3, do=0, fr_node = 0.5):\n",
        "  opt = tf.keras.optimizers.get(opt) #Set optimizer\n",
        "  K.set_value(opt.learning_rate, lr) #Set learning rate\n",
        "  \n",
        "  #Model architecture\n",
        "  input_layer = Input(shape=(xt.shape[1],))\n",
        "  hidden_layer = BatchNormalization()(input_layer)\n",
        "  hidden_layer = Dropout(do)(hidden_layer)\n",
        "  for n_layers in range(num_layers):\n",
        "    hidden_layer = Dense(int(hl_node*(fr_node**n_layers)), activation = af)(hidden_layer)\n",
        "    hidden_layer = BatchNormalization()(hidden_layer)\n",
        "    hidden_layer = Dropout(do)(hidden_layer)\n",
        "  output_layer = Dense(1, activation = \"sigmoid\")(hidden_layer)\n",
        "\n",
        "  dnn = Model(input_layer, output_layer)\n",
        "\n",
        "  #Using Weight Generated from SAE (if weights are provided)\n",
        "  if sae_weights is not None:\n",
        "    weights = sae_weights\n",
        "    dnn_dense = [layer_name for layer_name in dnn.layers if \"dense\" in layer_name.name]\n",
        "    for weight_from,weight_to in list(zip(weights,dnn_dense)):\n",
        "      weight_to.set_weights(weight_from)\n",
        "\n",
        "  #Compile model\n",
        "  dnn.compile(optimizer=opt, loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return dnn\n",
        "\n",
        "# def sae_dnn_model(xt, xv = None, EPOCHS = 50,BATCH_SIZE = 32,\n",
        "#                   hl_node = 1024, lr = 0.01, af = \"relu\", num_layers=3):\n",
        "#   params = {\n",
        "#       \"hl_node\" : hl_node,\n",
        "#       \"lr\" : lr,\n",
        "#       \"af\" : af,\n",
        "#       \"num_layers\":\n",
        "#       \"EPOCHS\" : EPOCHS,\n",
        "#       \"BATCH_SIZE\" : BATCH_SIZE,\n",
        "#   }\n",
        "#   sae_weights = sae_model(xt, xv, **params)\n",
        "#   return dnn_model(xt, sae_weights = sae_weights, **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqbO72XkSMQ8"
      },
      "source": [
        "## Generate Feature Vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install propy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfh28WJGTzNx",
        "outputId": "90052cd2-4813-4216-a1b6-816c6bf38ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: propy3 in /usr/local/lib/python3.7/dist-packages (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo9Wmh1gbFUl",
        "outputId": "594a6eab-59ed-427f-afd8-400185c768b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "881"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from propy.GetProteinFromUniprot import GetProteinSequence\n",
        "from propy.PyPro import GetProDes\n",
        "proteinsequence = GetProteinSequence(\"P48039\")\n",
        "\n",
        "target = GetProDes(proteinsequence).GetDPComp()\n",
        "target = [*target.values()]"
      ],
      "metadata": {
        "id": "G-iSNjVgbbmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVjMzLrXbxXm",
        "outputId": "af5d8885-3a73-441e-e608-dfee6bd326f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 1.15,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 1.43,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 1.15,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 1.15,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.86,\n",
              " 0.86,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.86,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.86,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.86,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.86,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.43,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.86,\n",
              " 0.86,\n",
              " 1.43,\n",
              " 1.15,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 1.15,\n",
              " 0.57,\n",
              " 0.86,\n",
              " 0.57,\n",
              " 0.57,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 1.43,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.86,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.15,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.57,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.72,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.86,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 1.15,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 1.15,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.86,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 0.57,\n",
              " 0.57,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 0.29,\n",
              " 1.43,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.86,\n",
              " 2.01,\n",
              " 0.86,\n",
              " 0.0,\n",
              " 0.86,\n",
              " 0.57,\n",
              " 1.15,\n",
              " 0.29,\n",
              " 0.0,\n",
              " 0.29,\n",
              " 1.72]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pubchempy as pcp\n",
        "ligand = pcp.Compound.from_cid(2920)\n",
        "temp = bin(int(ligand.fingerprint, 16))\n",
        "fp = temp[2:883]\n",
        "list0 = list(fp)\n",
        "for i in range(len(list0)):\n",
        "  list0[i] = float(list0[i])\n",
        "list0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df882ZWLU3Qs",
        "outputId": "64b222ed-2acb-4234-b222-978e311e66cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from propy.GetProteinFromUniprot import GetProteinSequence\n",
        "from propy.PyPro import GetProDes\n",
        "\n",
        "# download the protein sequence by uniprot id\n",
        "proteinsequence = GetProteinSequence(\"P48039\")\n",
        "\n",
        "target = GetProDes(proteinsequence).GetDPComp()\n",
        "\n"
      ],
      "metadata": {
        "id": "yIj_Lol4TuUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNf3p0cCvSI-"
      },
      "source": [
        "#Reading dataset\n",
        "cov_dti = pd.read_csv(\"./My Drive/Skripsi/Data/coronadata/cov_dti.csv\")\n",
        "cov_com_fp = pd.read_csv(\"./My Drive/Skripsi/Data/coronadata/cov_com_fingerprint.csv\")\n",
        "cov_pro_pf = pd.read_csv(\"./My Drive/Skripsi/Data/coronadata/cov_pro_pf.csv\")\n",
        "cov_pssm = pd.read_csv(\"./My Drive/Skripsi/Data/coronadata/cov_pssm.csv\")\n",
        "\n",
        "# cov_dti[\"Senyawa\"] = cov_dti[\"Senyawa\"].apply(lambda x : x.capitalize())\n",
        "# cov_dti.drop(columns = [\"Senyawa\"], inplace = True)\n",
        "cov_dti[\"CID_senyawa\"] = cov_dti[\"CID_senyawa\"].astype(\"str\")\n",
        "cov_com_fp[\"CID_senyawa\"] = cov_com_fp[\"CID_senyawa\"].astype(\"str\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM5zJb5VSEzv"
      },
      "source": [
        "#Filling missing values with median\n",
        "cov_com_fp = cov_com_fp.fillna(cov_com_fp.median())\n",
        "cov_pro_pf = cov_pro_pf.fillna(cov_pro_pf.median())\n",
        "cov_pssm = cov_pssm.fillna(cov_pssm.median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPByA9E4SAxz"
      },
      "source": [
        "#Get all interaction\n",
        "com_uni = cov_dti[\"CID_senyawa\"].unique()\n",
        "pro_uni = cov_dti[\"Protein\"].unique()\n",
        "# selected_pairs = cov_dti[cov_dti[\"CID_senyawa\"].isin(cov_com_feat[\"CID_senyawa\"].unique())]\n",
        "known_pair = [tuple(x) for x in cov_dti.to_numpy()]\n",
        "com2 = np.array([])\n",
        "pro2 = np.array([])\n",
        "for i in com_uni:\n",
        "    com2 = np.append(com2,np.array([i]*len(pro_uni)))\n",
        "pro2 = list(pro_uni)*len(com_uni)\n",
        "\n",
        "df = pd.DataFrame(list(zip(com2,pro2)), columns=[\"CID_senyawa\",\"Protein\"])\n",
        "\n",
        "df[\"class\"] = df.apply(lambda row: 1 if (row['CID_senyawa'], row['Protein']) in known_pair else 0, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXF-IKaNSUON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbef42e-1b87-4a38-a7c1-fd63a1650bb6"
      },
      "source": [
        "#Select features to be combined into feature vector\n",
        "cov_pro_combine = pd.merge(cov_pro_pf,cov_pssm,how=\"inner\",on=\"Protein\")\n",
        "select_com_feat = cov_com_fp\n",
        "# select_pro_feat = cov_pro_combine.loc[:,[\"Protein\"]+[i for i in cov_pro_combine.columns if any(z in i for z in [\"G1\",\"G2\",\"G3\",\"lag1\"])]]\n",
        "select_pro_feat = cov_pro_combine.loc[:,[\"Protein\"]+[i for i in cov_pro_combine.columns if any(z in i for z in [\"lag1\"])]]\n",
        "cov_feature_vector = pd.merge(pd.merge(df,select_com_feat,how=\"inner\",on=\"CID_senyawa\"),select_pro_feat,how=\"inner\",on=\"Protein\").drop_duplicates()\n",
        "\n",
        "#Print dataset information\n",
        "print(\"Known Interaction :\", len(cov_feature_vector[cov_feature_vector[\"class\"]==1]))\n",
        "print(\"All Interaction   :\", len(cov_feature_vector))\n",
        "print(\"Minority Class    : {:.3f}%\".format((len(cov_feature_vector[cov_feature_vector[\"class\"]==1])/len(cov_feature_vector))*100))\n",
        "print()\n",
        "print(\"Available Protein :\", cov_feature_vector[\"Protein\"].nunique())\n",
        "print(\"Protein Features  :\", select_pro_feat.shape[1]-1)\n",
        "print()\n",
        "print(\"Available Compound:\", cov_feature_vector[\"CID_senyawa\"].nunique())\n",
        "print(\"Compound Features :\", select_com_feat.shape[1]-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Known Interaction : 712\n",
            "All Interaction   : 39975\n",
            "Minority Class    : 1.781%\n",
            "\n",
            "Available Protein : 325\n",
            "Protein Features  : 400\n",
            "\n",
            "Available Compound: 123\n",
            "Compound Features : 881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cov_feature_vector[\"class\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL5kSBlyTHT2",
        "outputId": "063924d1-c780-410f-8c37-9d781a8dbba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        1\n",
              "1        1\n",
              "2        1\n",
              "3        1\n",
              "4        1\n",
              "        ..\n",
              "39970    0\n",
              "39971    0\n",
              "39972    0\n",
              "39973    0\n",
              "39974    1\n",
              "Name: class, Length: 39975, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB_vdvC3UC1z"
      },
      "source": [
        "X_all = cov_feature_vector.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"])\n",
        "y_all = cov_feature_vector[\"class\"]\n",
        "\n",
        "scaler_all = MinMaxScaler()\n",
        "X_all = pd.DataFrame(data = scaler_all.fit_transform(X_all), columns = X_all.columns)\n",
        "\n",
        "#Use all data, then split\n",
        "# #Data splitting, labelling, and normalizing\n",
        "# X = cov_feature_vector.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"])\n",
        "# y = cov_feature_vector[\"class\"]\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y, test_size = 0.1)\n",
        "\n",
        "# scaler = MinMaxScaler()\n",
        "# X_train_mm = pd.DataFrame(data = scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "# X_test_mm = pd.DataFrame(data = scaler.transform(X_test), columns = X_test.columns)\n",
        "\n",
        "# del X,y, X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2P8PcnWzzig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b6793f-5175-4188-c3cc-0621891509c4"
      },
      "source": [
        "#Random Undersampling\n",
        "X = cov_feature_vector.drop(columns = [\"class\"])\n",
        "y = cov_feature_vector.iloc[:,2]\n",
        "\n",
        "## 1:5 Ratio\n",
        "rus = RandomUnderSampler(random_state=42, sampling_strategy = 0.2)\n",
        "X_res, y_res = rus.fit_resample(X, y)\n",
        "rand_stratify = pd.concat([pd.DataFrame(X_res, columns = X.columns),pd.Series(y_res,name=\"class\")], axis = 1)\n",
        "\n",
        "#10% of unknown interaction\n",
        "# rand_stratify = pd.concat([cov_feature_vector[cov_feature_vector[\"class\"] == 1], cov_feature_vector[cov_feature_vector[\"class\"] == 0].sample(frac=0.1, random_state = 42)])\n",
        "\n",
        "X_train_rus = rand_stratify.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"])\n",
        "y_train_rus = rand_stratify[\"class\"]\n",
        "\n",
        "X_test_rus = X[~X.isin(pd.DataFrame(rand_stratify.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"]), columns = X.columns))].dropna().reset_index(drop = True)\n",
        "y_test_rus = [0]*X_test_rus.shape[0]\n",
        "\n",
        "rus_scaler = MinMaxScaler()\n",
        "X_train_rus_mm = pd.DataFrame(data = rus_scaler.fit_transform(X_train_rus), columns = X_train_rus.columns)\n",
        "X_test_rus_mm = pd.DataFrame(data = rus_scaler.transform(X_test_rus.drop(columns = [\"CID_senyawa\", \"Protein\"])), columns = X_test_rus.drop(columns = [\"CID_senyawa\", \"Protein\"]).columns)\n",
        "\n",
        "#Print dataset information\n",
        "print(\"Known Interaction  :\", len(rand_stratify[rand_stratify[\"class\"]==1]))\n",
        "print(\"Unknown Interaction:\", len(rand_stratify[rand_stratify[\"class\"]==0]))\n",
        "print()\n",
        "# print(\"Test data :\", len(X_test_rus))\n",
        "\n",
        "#Data splitting, labelling, and normalizing\n",
        "X = rand_stratify.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"])\n",
        "y = rand_stratify[\"class\"]\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y, test_size = 0.2)\n",
        "\n",
        "scaler = MinMaxScaler(feature_range = (0,1))\n",
        "X_train_mm = pd.DataFrame(data = scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "X_test_mm = pd.DataFrame(data = scaler.transform(X_test), columns = X_test.columns)\n",
        "\n",
        "del X,y, X_train, X_test\n",
        "\n",
        "cov_feature_vector = rand_stratify.copy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Known Interaction  : 712\n",
            "Unknown Interaction: 3560\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-kEWeTdbhZA"
      },
      "source": [
        "# dti_faldi = cov_feature_vector[cov_feature_vector[\"class\"]==1][[\"Protein\",\"CID_senyawa\"]]\n",
        "# dti_nabila = pd.read_csv(\"./My Drive/Skripsi/Data/coronadata/dti_latih.csv\")\n",
        "# cov_dti_nabila = pd.read_excel(\"./My Drive/Skripsi/Data/coronadata/cov_dti_nabila.xlsx\", header= None)\n",
        "# dti_nabila.columns = [\"CID_senyawa\",\"Protein\"]\n",
        "# cov_dti_nabila.columns = [\"CID_senyawa\",\"Senyawa\",\"Protein\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWsd6SUK2rlF"
      },
      "source": [
        "# sorted([i for i in dti_nabila[\"Protein\"].unique() if i not in dti_faldi[\"Protein\"].values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to9wYeHj3XDi"
      },
      "source": [
        "# [i for i in cov_dti_nabila[\"Protein\"].unique() if str(i) not in cov_dti[\"Protein\"].values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfRDggNnzg22"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0btQ9hltlRzq"
      },
      "source": [
        "|-activation: relu\n",
        "|-batch_size: 32\n",
        "|-learning_rate: 0.0001\n",
        "|-num_layers: 2\n",
        "|-units: 1024\n",
        "\n",
        "|-activation: relu\n",
        "|-batch_size: 32\n",
        "|-learning_rate: 0.0001\n",
        "|-num_layers: 2\n",
        "|-units: 1024\n",
        "\n",
        "|-activation: relu\n",
        "|-batch_size: 32\n",
        "|-dropout_rate: 0.5\n",
        "|-learning_rate: 0.0001\n",
        "|-num_layers: 3\n",
        "|-optimizer: nadam\n",
        "|-units: 1024\n",
        "\n",
        "|-activation: relu\n",
        "|-batch_size: 8\n",
        "|-dropout_rate: 0.5\n",
        "|-learning_rate: 0.01\n",
        "|-num_layers: 2\n",
        "|-optimizer: adam\n",
        "|-units: 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJzXFMCWiiK-"
      },
      "source": [
        "## DC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuRQt9RsvfBm"
      },
      "source": [
        "# |-activation: relu\n",
        "# |-batch_size: 8\n",
        "# |-dropout_rate: 0.5\n",
        "# |-fraction_node: 0.5\n",
        "# |-learning_rate: 0.01\n",
        "# |-num_layers: 2\n",
        "# |-units: 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install propy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YLPqV8sm6aC",
        "outputId": "b6444990-313d-44c3-cff1-0c222a759f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting propy3\n",
            "  Downloading propy3-1.1.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 4.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: propy3\n",
            "Successfully installed propy3-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pubchempy as pcp\n",
        "from propy.GetProteinFromUniprot import GetProteinSequence\n",
        "from propy.PyPro import GetProDes\n",
        "import pandas as pd\n",
        "\n",
        "ligand = pcp.Compound.from_cid(323)\n",
        "temp = bin(int(ligand.fingerprint, 16))\n",
        "fp = temp[2:883]\n",
        "list0 = list(fp)\n",
        "for i in range(len(list0)):\n",
        "  list0[i] = float(list0[i])\n",
        "\n",
        "# download the protein sequence by uniprot id\n",
        "proteinsequence = GetProteinSequence(\"P48039\")\n",
        "\n",
        "target = GetProDes(proteinsequence).GetDPComp()\n",
        "target = [*target.values()]\n",
        "\n",
        "ligand_protein = list0 + target\n",
        "\n",
        "input0 = pd.DataFrame(ligand_protein).transpose()"
      ],
      "metadata": {
        "id": "xHugXCMwk5Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "rXFEQkrJnsXi",
        "outputId": "812ad233-3562-42eb-ed50-865cd68aa797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0     1     2     3     4     5     6     7     8     9     ...  1271  \\\n",
              "0   1.0   1.0   0.0   1.0   1.0   1.0   0.0   0.0   0.0   1.0  ...  2.01   \n",
              "\n",
              "   1272  1273  1274  1275  1276  1277  1278  1279  1280  \n",
              "0  0.86   0.0  0.86  0.57  1.15  0.29   0.0  0.29  1.72  \n",
              "\n",
              "[1 rows x 1281 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d620d27-621e-4236-932e-cff3625ac572\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1271</th>\n",
              "      <th>1272</th>\n",
              "      <th>1273</th>\n",
              "      <th>1274</th>\n",
              "      <th>1275</th>\n",
              "      <th>1276</th>\n",
              "      <th>1277</th>\n",
              "      <th>1278</th>\n",
              "      <th>1279</th>\n",
              "      <th>1280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.72</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 1281 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d620d27-621e-4236-932e-cff3625ac572')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d620d27-621e-4236-932e-cff3625ac572 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d620d27-621e-4236-932e-cff3625ac572');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "model = pickle.load(open('sae_dnn.pkl', 'rb'))\n",
        "# app = Flask(__name__)\n",
        "\n",
        "prediction = model.predict([[input0]])\n",
        "output = np.round(prediction[0], 10)\n",
        "output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlxhwA0Lj6zJ",
        "outputId": "3d9ccacf-ce1a-4d66-8c57-a3d04f2e49cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 166ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28095442"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_mm.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "SvgFPwTf-pDz",
        "outputId": "fd51a7e5-c32f-4e0c-832e-d560a3e17c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     1    2    3    4    5    6    7    8    9   10   11   12   13   14   15  \\\n",
              "0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  1.0   \n",
              "1  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0   \n",
              "2  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  1.0   \n",
              "3  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
              "4  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0   \n",
              "\n",
              "    16   17   18   19   20   21   22   23   24   25   26   27   28   29   30  \\\n",
              "0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2  1.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "    31   32   33   34   35   36   37   38   39   40   41   42   43   44   45  \\\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "    46   47   48   49   50   51   52   53   54   55   56   57   58   59   60  \\\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "    61   62   63   64   65   66   67   68   69   70   71   72   73   74   75  \\\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "    76   77   78   79   80   81   82   83   84   85   86   87   88   89   90  \\\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "    91   92   93   94   95   96   97   98   99  100  ...  K.Q.lag1  M.Q.lag1  \\\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.293024  0.259958   \n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.368258  0.440858   \n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.464665  0.407510   \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.356151  0.323848   \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.403063  0.449958   \n",
              "\n",
              "   F.Q.lag1  P.Q.lag1  S.Q.lag1  T.Q.lag1  W.Q.lag1  Y.Q.lag1  V.Q.lag1  \\\n",
              "0  0.357832  0.307980  0.429858  0.441594  0.390912  0.310321  0.427482   \n",
              "1  0.430362  0.382828  0.399812  0.482546  0.408355  0.359603  0.571978   \n",
              "2  0.436038  0.577985  0.543909  0.564992  0.529706  0.498388  0.392961   \n",
              "3  0.375526  0.435336  0.456549  0.488699  0.437241  0.362415  0.389957   \n",
              "4  0.350770  0.520756  0.489287  0.509985  0.278006  0.316818  0.547041   \n",
              "\n",
              "   G.E.lag1  H.E.lag1  I.E.lag1  L.E.lag1  K.E.lag1  M.E.lag1  F.E.lag1  \\\n",
              "0  0.335988  0.341895  0.349073  0.318503  0.295263  0.289157  0.412426   \n",
              "1  0.160757  0.357573  0.456936  0.455544  0.360728  0.412219  0.484881   \n",
              "2  0.323386  0.576413  0.306971  0.326416  0.470562  0.401028  0.492228   \n",
              "3  0.323367  0.401230  0.307764  0.288919  0.344982  0.302485  0.405584   \n",
              "4  0.369096  0.401864  0.461409  0.448920  0.388612  0.450627  0.397024   \n",
              "\n",
              "   P.E.lag1  S.E.lag1  T.E.lag1  W.E.lag1  Y.E.lag1  V.E.lag1  H.G.lag1  \\\n",
              "0  0.328731  0.431413  0.459299  0.471659  0.423459  0.429698  0.309307   \n",
              "1  0.389193  0.384071  0.483570  0.504223  0.481531  0.519515  0.230882   \n",
              "2  0.614773  0.547512  0.577924  0.629231  0.643751  0.373265  0.388905   \n",
              "3  0.457350  0.448096  0.489344  0.507953  0.464159  0.342955  0.333624   \n",
              "4  0.552608  0.477490  0.526173  0.341172  0.400439  0.535752  0.344968   \n",
              "\n",
              "   I.G.lag1  L.G.lag1  K.G.lag1  M.G.lag1  F.G.lag1  P.G.lag1  S.G.lag1  \\\n",
              "0  0.504998  0.500149  0.350310  0.540071  0.404682  0.525549  0.351516   \n",
              "1  0.423444  0.424797  0.253668  0.332688  0.485554  0.221924  0.148617   \n",
              "2  0.275296  0.293144  0.421412  0.338386  0.310634  0.451831  0.276123   \n",
              "3  0.420720  0.416769  0.372993  0.459656  0.386284  0.507646  0.292632   \n",
              "4  0.513681  0.502919  0.379740  0.566506  0.353809  0.519482  0.265099   \n",
              "\n",
              "   T.G.lag1  W.G.lag1  Y.G.lag1  V.G.lag1  I.H.lag1  L.H.lag1  K.H.lag1  \\\n",
              "0  0.396578  0.356215  0.310021  0.415934  0.324061  0.376089  0.323041   \n",
              "1  0.204649  0.444495  0.393819  0.244939  0.435198  0.491025  0.406147   \n",
              "2  0.313358  0.365117  0.343622  0.202100  0.272859  0.346088  0.530349   \n",
              "3  0.353496  0.388178  0.353803  0.302202  0.386928  0.420754  0.392978   \n",
              "4  0.322900  0.272683  0.294028  0.368903  0.376384  0.417110  0.412870   \n",
              "\n",
              "   M.H.lag1  F.H.lag1  P.H.lag1  S.H.lag1  T.H.lag1  W.H.lag1  Y.H.lag1  \\\n",
              "0  0.397421  0.358677  0.305232  0.486508  0.501232  0.347306  0.333130   \n",
              "1  0.486442  0.352008  0.486420  0.461834  0.580687  0.265604  0.326211   \n",
              "2  0.476506  0.432241  0.774840  0.670124  0.719831  0.475481  0.539555   \n",
              "3  0.469027  0.365261  0.518888  0.526753  0.595558  0.328163  0.372649   \n",
              "4  0.464390  0.312255  0.594530  0.541771  0.576914  0.219503  0.314002   \n",
              "\n",
              "   V.H.lag1  L.I.lag1  K.I.lag1  M.I.lag1  F.I.lag1  P.I.lag1  S.I.lag1  \\\n",
              "0  0.398452  0.403419  0.594172  0.395671  0.394843  0.535496  0.526429   \n",
              "1  0.571300  0.512895  0.698993  0.471326  0.608416  0.505527  0.592855   \n",
              "2  0.363182  0.288235  0.465235  0.287812  0.336948  0.364798  0.315423   \n",
              "3  0.483625  0.365281  0.517668  0.359044  0.408024  0.389546  0.509852   \n",
              "4  0.486530  0.353437  0.695450  0.352011  0.422756  0.422714  0.528473   \n",
              "\n",
              "   T.I.lag1  W.I.lag1  Y.I.lag1  V.I.lag1  K.L.lag1  M.L.lag1  F.L.lag1  \\\n",
              "0  0.407957  0.470391  0.492347  0.303987  0.612889  0.367244  0.408634   \n",
              "1  0.477241  0.752372  0.685787  0.364605  0.737153  0.455916  0.629120   \n",
              "2  0.246157  0.467656  0.478275  0.212325  0.483119  0.286514  0.381126   \n",
              "3  0.385612  0.549306  0.502835  0.269076  0.552385  0.341959  0.426548   \n",
              "4  0.338006  0.573526  0.525212  0.268139  0.744637  0.329811  0.431025   \n",
              "\n",
              "   P.L.lag1  S.L.lag1  T.L.lag1  W.L.lag1  Y.L.lag1  V.L.lag1  M.K.lag1  \\\n",
              "0  0.528212  0.587002  0.404216  0.493966  0.498415  0.293241  0.348787   \n",
              "1  0.504889  0.671004  0.481935  0.770142  0.704314  0.361824  0.634365   \n",
              "2  0.360980  0.356215  0.245867  0.512228  0.521883  0.219144  0.535568   \n",
              "3  0.371546  0.570679  0.381531  0.568897  0.521190  0.263080  0.436828   \n",
              "4  0.395698  0.598685  0.329974  0.577654  0.537698  0.261555  0.589275   \n",
              "\n",
              "   F.K.lag1  P.K.lag1  S.K.lag1  T.K.lag1  W.K.lag1  Y.K.lag1  V.K.lag1  \\\n",
              "0  0.532374  0.278396  0.383469  0.393527  0.472056  0.476658  0.521534   \n",
              "1  0.683049  0.369158  0.368038  0.452183  0.496290  0.612162  0.724237   \n",
              "2  0.622428  0.516997  0.472975  0.494683  0.538640  0.761870  0.507282   \n",
              "3  0.559997  0.409266  0.410917  0.447181  0.496515  0.576722  0.482713   \n",
              "4  0.480382  0.506023  0.446055  0.468953  0.314262  0.462790  0.674199   \n",
              "\n",
              "   F.M.lag1  P.M.lag1  S.M.lag1  T.M.lag1  W.M.lag1  Y.M.lag1  V.M.lag1  \\\n",
              "0  0.445078  0.431471  0.568222  0.424242  0.518344  0.559812  0.322380   \n",
              "1  0.612552  0.415248  0.604294  0.493717  0.715668  0.722561  0.380826   \n",
              "2  0.441643  0.366720  0.412923  0.307546  0.575143  0.647035  0.253934   \n",
              "3  0.471254  0.347140  0.617217  0.434469  0.605296  0.607878  0.296700   \n",
              "4  0.457365  0.368905  0.624881  0.361094  0.557365  0.600378  0.297390   \n",
              "\n",
              "   P.F.lag1  S.F.lag1  T.F.lag1  W.F.lag1  Y.F.lag1  V.F.lag1  S.P.lag1  \\\n",
              "0  0.714478  0.524807  0.514106  0.467660  0.447465  0.357204  0.458585   \n",
              "1  0.847957  0.638372  0.702471  0.642224  0.576598  0.471055  0.363916   \n",
              "2  0.657767  0.414720  0.422107  0.563587  0.546737  0.322804  0.514988   \n",
              "3  0.572229  0.575193  0.564863  0.522754  0.482887  0.343052  0.425758   \n",
              "4  0.523286  0.457028  0.381458  0.496923  0.454194  0.282179  0.492311   \n",
              "\n",
              "   T.P.lag1  W.P.lag1  Y.P.lag1  V.P.lag1  T.S.lag1  W.S.lag1  Y.S.lag1  \\\n",
              "0  0.303262  0.730267  0.669139  0.318119  0.376834  0.514729  0.421592   \n",
              "1  0.248980  0.587311  0.590625  0.285251  0.283089  0.503768  0.444269   \n",
              "2  0.274087  0.704773  0.767502  0.195161  0.382056  0.524653  0.547338   \n",
              "3  0.273873  0.762336  0.682424  0.234961  0.385792  0.472544  0.414248   \n",
              "4  0.306488  0.408899  0.455587  0.258924  0.305683  0.331059  0.292795   \n",
              "\n",
              "   V.S.lag1  W.T.lag1  Y.T.lag1  V.T.lag1  Y.W.lag1  V.W.lag1  V.Y.lag1  \n",
              "0  0.431067  0.571747  0.519178  0.513422  0.421268  0.394813  0.429494  \n",
              "1  0.384909  0.604877  0.575201  0.559276  0.508245  0.518983  0.574254  \n",
              "2  0.249831  0.504118  0.550609  0.329275  0.586693  0.427759  0.444450  \n",
              "3  0.334846  0.502924  0.487503  0.463331  0.480801  0.413950  0.464110  \n",
              "4  0.394347  0.340689  0.365051  0.446110  0.402498  0.263965  0.356758  \n",
              "\n",
              "[5 rows x 1281 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a93e52f1-e63f-4a17-a9ff-acce52c5a948\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>...</th>\n",
              "      <th>K.Q.lag1</th>\n",
              "      <th>M.Q.lag1</th>\n",
              "      <th>F.Q.lag1</th>\n",
              "      <th>P.Q.lag1</th>\n",
              "      <th>S.Q.lag1</th>\n",
              "      <th>T.Q.lag1</th>\n",
              "      <th>W.Q.lag1</th>\n",
              "      <th>Y.Q.lag1</th>\n",
              "      <th>V.Q.lag1</th>\n",
              "      <th>G.E.lag1</th>\n",
              "      <th>H.E.lag1</th>\n",
              "      <th>I.E.lag1</th>\n",
              "      <th>L.E.lag1</th>\n",
              "      <th>K.E.lag1</th>\n",
              "      <th>M.E.lag1</th>\n",
              "      <th>F.E.lag1</th>\n",
              "      <th>P.E.lag1</th>\n",
              "      <th>S.E.lag1</th>\n",
              "      <th>T.E.lag1</th>\n",
              "      <th>W.E.lag1</th>\n",
              "      <th>Y.E.lag1</th>\n",
              "      <th>V.E.lag1</th>\n",
              "      <th>H.G.lag1</th>\n",
              "      <th>I.G.lag1</th>\n",
              "      <th>L.G.lag1</th>\n",
              "      <th>K.G.lag1</th>\n",
              "      <th>M.G.lag1</th>\n",
              "      <th>F.G.lag1</th>\n",
              "      <th>P.G.lag1</th>\n",
              "      <th>S.G.lag1</th>\n",
              "      <th>T.G.lag1</th>\n",
              "      <th>W.G.lag1</th>\n",
              "      <th>Y.G.lag1</th>\n",
              "      <th>V.G.lag1</th>\n",
              "      <th>I.H.lag1</th>\n",
              "      <th>L.H.lag1</th>\n",
              "      <th>K.H.lag1</th>\n",
              "      <th>M.H.lag1</th>\n",
              "      <th>F.H.lag1</th>\n",
              "      <th>P.H.lag1</th>\n",
              "      <th>S.H.lag1</th>\n",
              "      <th>T.H.lag1</th>\n",
              "      <th>W.H.lag1</th>\n",
              "      <th>Y.H.lag1</th>\n",
              "      <th>V.H.lag1</th>\n",
              "      <th>L.I.lag1</th>\n",
              "      <th>K.I.lag1</th>\n",
              "      <th>M.I.lag1</th>\n",
              "      <th>F.I.lag1</th>\n",
              "      <th>P.I.lag1</th>\n",
              "      <th>S.I.lag1</th>\n",
              "      <th>T.I.lag1</th>\n",
              "      <th>W.I.lag1</th>\n",
              "      <th>Y.I.lag1</th>\n",
              "      <th>V.I.lag1</th>\n",
              "      <th>K.L.lag1</th>\n",
              "      <th>M.L.lag1</th>\n",
              "      <th>F.L.lag1</th>\n",
              "      <th>P.L.lag1</th>\n",
              "      <th>S.L.lag1</th>\n",
              "      <th>T.L.lag1</th>\n",
              "      <th>W.L.lag1</th>\n",
              "      <th>Y.L.lag1</th>\n",
              "      <th>V.L.lag1</th>\n",
              "      <th>M.K.lag1</th>\n",
              "      <th>F.K.lag1</th>\n",
              "      <th>P.K.lag1</th>\n",
              "      <th>S.K.lag1</th>\n",
              "      <th>T.K.lag1</th>\n",
              "      <th>W.K.lag1</th>\n",
              "      <th>Y.K.lag1</th>\n",
              "      <th>V.K.lag1</th>\n",
              "      <th>F.M.lag1</th>\n",
              "      <th>P.M.lag1</th>\n",
              "      <th>S.M.lag1</th>\n",
              "      <th>T.M.lag1</th>\n",
              "      <th>W.M.lag1</th>\n",
              "      <th>Y.M.lag1</th>\n",
              "      <th>V.M.lag1</th>\n",
              "      <th>P.F.lag1</th>\n",
              "      <th>S.F.lag1</th>\n",
              "      <th>T.F.lag1</th>\n",
              "      <th>W.F.lag1</th>\n",
              "      <th>Y.F.lag1</th>\n",
              "      <th>V.F.lag1</th>\n",
              "      <th>S.P.lag1</th>\n",
              "      <th>T.P.lag1</th>\n",
              "      <th>W.P.lag1</th>\n",
              "      <th>Y.P.lag1</th>\n",
              "      <th>V.P.lag1</th>\n",
              "      <th>T.S.lag1</th>\n",
              "      <th>W.S.lag1</th>\n",
              "      <th>Y.S.lag1</th>\n",
              "      <th>V.S.lag1</th>\n",
              "      <th>W.T.lag1</th>\n",
              "      <th>Y.T.lag1</th>\n",
              "      <th>V.T.lag1</th>\n",
              "      <th>Y.W.lag1</th>\n",
              "      <th>V.W.lag1</th>\n",
              "      <th>V.Y.lag1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.293024</td>\n",
              "      <td>0.259958</td>\n",
              "      <td>0.357832</td>\n",
              "      <td>0.307980</td>\n",
              "      <td>0.429858</td>\n",
              "      <td>0.441594</td>\n",
              "      <td>0.390912</td>\n",
              "      <td>0.310321</td>\n",
              "      <td>0.427482</td>\n",
              "      <td>0.335988</td>\n",
              "      <td>0.341895</td>\n",
              "      <td>0.349073</td>\n",
              "      <td>0.318503</td>\n",
              "      <td>0.295263</td>\n",
              "      <td>0.289157</td>\n",
              "      <td>0.412426</td>\n",
              "      <td>0.328731</td>\n",
              "      <td>0.431413</td>\n",
              "      <td>0.459299</td>\n",
              "      <td>0.471659</td>\n",
              "      <td>0.423459</td>\n",
              "      <td>0.429698</td>\n",
              "      <td>0.309307</td>\n",
              "      <td>0.504998</td>\n",
              "      <td>0.500149</td>\n",
              "      <td>0.350310</td>\n",
              "      <td>0.540071</td>\n",
              "      <td>0.404682</td>\n",
              "      <td>0.525549</td>\n",
              "      <td>0.351516</td>\n",
              "      <td>0.396578</td>\n",
              "      <td>0.356215</td>\n",
              "      <td>0.310021</td>\n",
              "      <td>0.415934</td>\n",
              "      <td>0.324061</td>\n",
              "      <td>0.376089</td>\n",
              "      <td>0.323041</td>\n",
              "      <td>0.397421</td>\n",
              "      <td>0.358677</td>\n",
              "      <td>0.305232</td>\n",
              "      <td>0.486508</td>\n",
              "      <td>0.501232</td>\n",
              "      <td>0.347306</td>\n",
              "      <td>0.333130</td>\n",
              "      <td>0.398452</td>\n",
              "      <td>0.403419</td>\n",
              "      <td>0.594172</td>\n",
              "      <td>0.395671</td>\n",
              "      <td>0.394843</td>\n",
              "      <td>0.535496</td>\n",
              "      <td>0.526429</td>\n",
              "      <td>0.407957</td>\n",
              "      <td>0.470391</td>\n",
              "      <td>0.492347</td>\n",
              "      <td>0.303987</td>\n",
              "      <td>0.612889</td>\n",
              "      <td>0.367244</td>\n",
              "      <td>0.408634</td>\n",
              "      <td>0.528212</td>\n",
              "      <td>0.587002</td>\n",
              "      <td>0.404216</td>\n",
              "      <td>0.493966</td>\n",
              "      <td>0.498415</td>\n",
              "      <td>0.293241</td>\n",
              "      <td>0.348787</td>\n",
              "      <td>0.532374</td>\n",
              "      <td>0.278396</td>\n",
              "      <td>0.383469</td>\n",
              "      <td>0.393527</td>\n",
              "      <td>0.472056</td>\n",
              "      <td>0.476658</td>\n",
              "      <td>0.521534</td>\n",
              "      <td>0.445078</td>\n",
              "      <td>0.431471</td>\n",
              "      <td>0.568222</td>\n",
              "      <td>0.424242</td>\n",
              "      <td>0.518344</td>\n",
              "      <td>0.559812</td>\n",
              "      <td>0.322380</td>\n",
              "      <td>0.714478</td>\n",
              "      <td>0.524807</td>\n",
              "      <td>0.514106</td>\n",
              "      <td>0.467660</td>\n",
              "      <td>0.447465</td>\n",
              "      <td>0.357204</td>\n",
              "      <td>0.458585</td>\n",
              "      <td>0.303262</td>\n",
              "      <td>0.730267</td>\n",
              "      <td>0.669139</td>\n",
              "      <td>0.318119</td>\n",
              "      <td>0.376834</td>\n",
              "      <td>0.514729</td>\n",
              "      <td>0.421592</td>\n",
              "      <td>0.431067</td>\n",
              "      <td>0.571747</td>\n",
              "      <td>0.519178</td>\n",
              "      <td>0.513422</td>\n",
              "      <td>0.421268</td>\n",
              "      <td>0.394813</td>\n",
              "      <td>0.429494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.368258</td>\n",
              "      <td>0.440858</td>\n",
              "      <td>0.430362</td>\n",
              "      <td>0.382828</td>\n",
              "      <td>0.399812</td>\n",
              "      <td>0.482546</td>\n",
              "      <td>0.408355</td>\n",
              "      <td>0.359603</td>\n",
              "      <td>0.571978</td>\n",
              "      <td>0.160757</td>\n",
              "      <td>0.357573</td>\n",
              "      <td>0.456936</td>\n",
              "      <td>0.455544</td>\n",
              "      <td>0.360728</td>\n",
              "      <td>0.412219</td>\n",
              "      <td>0.484881</td>\n",
              "      <td>0.389193</td>\n",
              "      <td>0.384071</td>\n",
              "      <td>0.483570</td>\n",
              "      <td>0.504223</td>\n",
              "      <td>0.481531</td>\n",
              "      <td>0.519515</td>\n",
              "      <td>0.230882</td>\n",
              "      <td>0.423444</td>\n",
              "      <td>0.424797</td>\n",
              "      <td>0.253668</td>\n",
              "      <td>0.332688</td>\n",
              "      <td>0.485554</td>\n",
              "      <td>0.221924</td>\n",
              "      <td>0.148617</td>\n",
              "      <td>0.204649</td>\n",
              "      <td>0.444495</td>\n",
              "      <td>0.393819</td>\n",
              "      <td>0.244939</td>\n",
              "      <td>0.435198</td>\n",
              "      <td>0.491025</td>\n",
              "      <td>0.406147</td>\n",
              "      <td>0.486442</td>\n",
              "      <td>0.352008</td>\n",
              "      <td>0.486420</td>\n",
              "      <td>0.461834</td>\n",
              "      <td>0.580687</td>\n",
              "      <td>0.265604</td>\n",
              "      <td>0.326211</td>\n",
              "      <td>0.571300</td>\n",
              "      <td>0.512895</td>\n",
              "      <td>0.698993</td>\n",
              "      <td>0.471326</td>\n",
              "      <td>0.608416</td>\n",
              "      <td>0.505527</td>\n",
              "      <td>0.592855</td>\n",
              "      <td>0.477241</td>\n",
              "      <td>0.752372</td>\n",
              "      <td>0.685787</td>\n",
              "      <td>0.364605</td>\n",
              "      <td>0.737153</td>\n",
              "      <td>0.455916</td>\n",
              "      <td>0.629120</td>\n",
              "      <td>0.504889</td>\n",
              "      <td>0.671004</td>\n",
              "      <td>0.481935</td>\n",
              "      <td>0.770142</td>\n",
              "      <td>0.704314</td>\n",
              "      <td>0.361824</td>\n",
              "      <td>0.634365</td>\n",
              "      <td>0.683049</td>\n",
              "      <td>0.369158</td>\n",
              "      <td>0.368038</td>\n",
              "      <td>0.452183</td>\n",
              "      <td>0.496290</td>\n",
              "      <td>0.612162</td>\n",
              "      <td>0.724237</td>\n",
              "      <td>0.612552</td>\n",
              "      <td>0.415248</td>\n",
              "      <td>0.604294</td>\n",
              "      <td>0.493717</td>\n",
              "      <td>0.715668</td>\n",
              "      <td>0.722561</td>\n",
              "      <td>0.380826</td>\n",
              "      <td>0.847957</td>\n",
              "      <td>0.638372</td>\n",
              "      <td>0.702471</td>\n",
              "      <td>0.642224</td>\n",
              "      <td>0.576598</td>\n",
              "      <td>0.471055</td>\n",
              "      <td>0.363916</td>\n",
              "      <td>0.248980</td>\n",
              "      <td>0.587311</td>\n",
              "      <td>0.590625</td>\n",
              "      <td>0.285251</td>\n",
              "      <td>0.283089</td>\n",
              "      <td>0.503768</td>\n",
              "      <td>0.444269</td>\n",
              "      <td>0.384909</td>\n",
              "      <td>0.604877</td>\n",
              "      <td>0.575201</td>\n",
              "      <td>0.559276</td>\n",
              "      <td>0.508245</td>\n",
              "      <td>0.518983</td>\n",
              "      <td>0.574254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.464665</td>\n",
              "      <td>0.407510</td>\n",
              "      <td>0.436038</td>\n",
              "      <td>0.577985</td>\n",
              "      <td>0.543909</td>\n",
              "      <td>0.564992</td>\n",
              "      <td>0.529706</td>\n",
              "      <td>0.498388</td>\n",
              "      <td>0.392961</td>\n",
              "      <td>0.323386</td>\n",
              "      <td>0.576413</td>\n",
              "      <td>0.306971</td>\n",
              "      <td>0.326416</td>\n",
              "      <td>0.470562</td>\n",
              "      <td>0.401028</td>\n",
              "      <td>0.492228</td>\n",
              "      <td>0.614773</td>\n",
              "      <td>0.547512</td>\n",
              "      <td>0.577924</td>\n",
              "      <td>0.629231</td>\n",
              "      <td>0.643751</td>\n",
              "      <td>0.373265</td>\n",
              "      <td>0.388905</td>\n",
              "      <td>0.275296</td>\n",
              "      <td>0.293144</td>\n",
              "      <td>0.421412</td>\n",
              "      <td>0.338386</td>\n",
              "      <td>0.310634</td>\n",
              "      <td>0.451831</td>\n",
              "      <td>0.276123</td>\n",
              "      <td>0.313358</td>\n",
              "      <td>0.365117</td>\n",
              "      <td>0.343622</td>\n",
              "      <td>0.202100</td>\n",
              "      <td>0.272859</td>\n",
              "      <td>0.346088</td>\n",
              "      <td>0.530349</td>\n",
              "      <td>0.476506</td>\n",
              "      <td>0.432241</td>\n",
              "      <td>0.774840</td>\n",
              "      <td>0.670124</td>\n",
              "      <td>0.719831</td>\n",
              "      <td>0.475481</td>\n",
              "      <td>0.539555</td>\n",
              "      <td>0.363182</td>\n",
              "      <td>0.288235</td>\n",
              "      <td>0.465235</td>\n",
              "      <td>0.287812</td>\n",
              "      <td>0.336948</td>\n",
              "      <td>0.364798</td>\n",
              "      <td>0.315423</td>\n",
              "      <td>0.246157</td>\n",
              "      <td>0.467656</td>\n",
              "      <td>0.478275</td>\n",
              "      <td>0.212325</td>\n",
              "      <td>0.483119</td>\n",
              "      <td>0.286514</td>\n",
              "      <td>0.381126</td>\n",
              "      <td>0.360980</td>\n",
              "      <td>0.356215</td>\n",
              "      <td>0.245867</td>\n",
              "      <td>0.512228</td>\n",
              "      <td>0.521883</td>\n",
              "      <td>0.219144</td>\n",
              "      <td>0.535568</td>\n",
              "      <td>0.622428</td>\n",
              "      <td>0.516997</td>\n",
              "      <td>0.472975</td>\n",
              "      <td>0.494683</td>\n",
              "      <td>0.538640</td>\n",
              "      <td>0.761870</td>\n",
              "      <td>0.507282</td>\n",
              "      <td>0.441643</td>\n",
              "      <td>0.366720</td>\n",
              "      <td>0.412923</td>\n",
              "      <td>0.307546</td>\n",
              "      <td>0.575143</td>\n",
              "      <td>0.647035</td>\n",
              "      <td>0.253934</td>\n",
              "      <td>0.657767</td>\n",
              "      <td>0.414720</td>\n",
              "      <td>0.422107</td>\n",
              "      <td>0.563587</td>\n",
              "      <td>0.546737</td>\n",
              "      <td>0.322804</td>\n",
              "      <td>0.514988</td>\n",
              "      <td>0.274087</td>\n",
              "      <td>0.704773</td>\n",
              "      <td>0.767502</td>\n",
              "      <td>0.195161</td>\n",
              "      <td>0.382056</td>\n",
              "      <td>0.524653</td>\n",
              "      <td>0.547338</td>\n",
              "      <td>0.249831</td>\n",
              "      <td>0.504118</td>\n",
              "      <td>0.550609</td>\n",
              "      <td>0.329275</td>\n",
              "      <td>0.586693</td>\n",
              "      <td>0.427759</td>\n",
              "      <td>0.444450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.356151</td>\n",
              "      <td>0.323848</td>\n",
              "      <td>0.375526</td>\n",
              "      <td>0.435336</td>\n",
              "      <td>0.456549</td>\n",
              "      <td>0.488699</td>\n",
              "      <td>0.437241</td>\n",
              "      <td>0.362415</td>\n",
              "      <td>0.389957</td>\n",
              "      <td>0.323367</td>\n",
              "      <td>0.401230</td>\n",
              "      <td>0.307764</td>\n",
              "      <td>0.288919</td>\n",
              "      <td>0.344982</td>\n",
              "      <td>0.302485</td>\n",
              "      <td>0.405584</td>\n",
              "      <td>0.457350</td>\n",
              "      <td>0.448096</td>\n",
              "      <td>0.489344</td>\n",
              "      <td>0.507953</td>\n",
              "      <td>0.464159</td>\n",
              "      <td>0.342955</td>\n",
              "      <td>0.333624</td>\n",
              "      <td>0.420720</td>\n",
              "      <td>0.416769</td>\n",
              "      <td>0.372993</td>\n",
              "      <td>0.459656</td>\n",
              "      <td>0.386284</td>\n",
              "      <td>0.507646</td>\n",
              "      <td>0.292632</td>\n",
              "      <td>0.353496</td>\n",
              "      <td>0.388178</td>\n",
              "      <td>0.353803</td>\n",
              "      <td>0.302202</td>\n",
              "      <td>0.386928</td>\n",
              "      <td>0.420754</td>\n",
              "      <td>0.392978</td>\n",
              "      <td>0.469027</td>\n",
              "      <td>0.365261</td>\n",
              "      <td>0.518888</td>\n",
              "      <td>0.526753</td>\n",
              "      <td>0.595558</td>\n",
              "      <td>0.328163</td>\n",
              "      <td>0.372649</td>\n",
              "      <td>0.483625</td>\n",
              "      <td>0.365281</td>\n",
              "      <td>0.517668</td>\n",
              "      <td>0.359044</td>\n",
              "      <td>0.408024</td>\n",
              "      <td>0.389546</td>\n",
              "      <td>0.509852</td>\n",
              "      <td>0.385612</td>\n",
              "      <td>0.549306</td>\n",
              "      <td>0.502835</td>\n",
              "      <td>0.269076</td>\n",
              "      <td>0.552385</td>\n",
              "      <td>0.341959</td>\n",
              "      <td>0.426548</td>\n",
              "      <td>0.371546</td>\n",
              "      <td>0.570679</td>\n",
              "      <td>0.381531</td>\n",
              "      <td>0.568897</td>\n",
              "      <td>0.521190</td>\n",
              "      <td>0.263080</td>\n",
              "      <td>0.436828</td>\n",
              "      <td>0.559997</td>\n",
              "      <td>0.409266</td>\n",
              "      <td>0.410917</td>\n",
              "      <td>0.447181</td>\n",
              "      <td>0.496515</td>\n",
              "      <td>0.576722</td>\n",
              "      <td>0.482713</td>\n",
              "      <td>0.471254</td>\n",
              "      <td>0.347140</td>\n",
              "      <td>0.617217</td>\n",
              "      <td>0.434469</td>\n",
              "      <td>0.605296</td>\n",
              "      <td>0.607878</td>\n",
              "      <td>0.296700</td>\n",
              "      <td>0.572229</td>\n",
              "      <td>0.575193</td>\n",
              "      <td>0.564863</td>\n",
              "      <td>0.522754</td>\n",
              "      <td>0.482887</td>\n",
              "      <td>0.343052</td>\n",
              "      <td>0.425758</td>\n",
              "      <td>0.273873</td>\n",
              "      <td>0.762336</td>\n",
              "      <td>0.682424</td>\n",
              "      <td>0.234961</td>\n",
              "      <td>0.385792</td>\n",
              "      <td>0.472544</td>\n",
              "      <td>0.414248</td>\n",
              "      <td>0.334846</td>\n",
              "      <td>0.502924</td>\n",
              "      <td>0.487503</td>\n",
              "      <td>0.463331</td>\n",
              "      <td>0.480801</td>\n",
              "      <td>0.413950</td>\n",
              "      <td>0.464110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.403063</td>\n",
              "      <td>0.449958</td>\n",
              "      <td>0.350770</td>\n",
              "      <td>0.520756</td>\n",
              "      <td>0.489287</td>\n",
              "      <td>0.509985</td>\n",
              "      <td>0.278006</td>\n",
              "      <td>0.316818</td>\n",
              "      <td>0.547041</td>\n",
              "      <td>0.369096</td>\n",
              "      <td>0.401864</td>\n",
              "      <td>0.461409</td>\n",
              "      <td>0.448920</td>\n",
              "      <td>0.388612</td>\n",
              "      <td>0.450627</td>\n",
              "      <td>0.397024</td>\n",
              "      <td>0.552608</td>\n",
              "      <td>0.477490</td>\n",
              "      <td>0.526173</td>\n",
              "      <td>0.341172</td>\n",
              "      <td>0.400439</td>\n",
              "      <td>0.535752</td>\n",
              "      <td>0.344968</td>\n",
              "      <td>0.513681</td>\n",
              "      <td>0.502919</td>\n",
              "      <td>0.379740</td>\n",
              "      <td>0.566506</td>\n",
              "      <td>0.353809</td>\n",
              "      <td>0.519482</td>\n",
              "      <td>0.265099</td>\n",
              "      <td>0.322900</td>\n",
              "      <td>0.272683</td>\n",
              "      <td>0.294028</td>\n",
              "      <td>0.368903</td>\n",
              "      <td>0.376384</td>\n",
              "      <td>0.417110</td>\n",
              "      <td>0.412870</td>\n",
              "      <td>0.464390</td>\n",
              "      <td>0.312255</td>\n",
              "      <td>0.594530</td>\n",
              "      <td>0.541771</td>\n",
              "      <td>0.576914</td>\n",
              "      <td>0.219503</td>\n",
              "      <td>0.314002</td>\n",
              "      <td>0.486530</td>\n",
              "      <td>0.353437</td>\n",
              "      <td>0.695450</td>\n",
              "      <td>0.352011</td>\n",
              "      <td>0.422756</td>\n",
              "      <td>0.422714</td>\n",
              "      <td>0.528473</td>\n",
              "      <td>0.338006</td>\n",
              "      <td>0.573526</td>\n",
              "      <td>0.525212</td>\n",
              "      <td>0.268139</td>\n",
              "      <td>0.744637</td>\n",
              "      <td>0.329811</td>\n",
              "      <td>0.431025</td>\n",
              "      <td>0.395698</td>\n",
              "      <td>0.598685</td>\n",
              "      <td>0.329974</td>\n",
              "      <td>0.577654</td>\n",
              "      <td>0.537698</td>\n",
              "      <td>0.261555</td>\n",
              "      <td>0.589275</td>\n",
              "      <td>0.480382</td>\n",
              "      <td>0.506023</td>\n",
              "      <td>0.446055</td>\n",
              "      <td>0.468953</td>\n",
              "      <td>0.314262</td>\n",
              "      <td>0.462790</td>\n",
              "      <td>0.674199</td>\n",
              "      <td>0.457365</td>\n",
              "      <td>0.368905</td>\n",
              "      <td>0.624881</td>\n",
              "      <td>0.361094</td>\n",
              "      <td>0.557365</td>\n",
              "      <td>0.600378</td>\n",
              "      <td>0.297390</td>\n",
              "      <td>0.523286</td>\n",
              "      <td>0.457028</td>\n",
              "      <td>0.381458</td>\n",
              "      <td>0.496923</td>\n",
              "      <td>0.454194</td>\n",
              "      <td>0.282179</td>\n",
              "      <td>0.492311</td>\n",
              "      <td>0.306488</td>\n",
              "      <td>0.408899</td>\n",
              "      <td>0.455587</td>\n",
              "      <td>0.258924</td>\n",
              "      <td>0.305683</td>\n",
              "      <td>0.331059</td>\n",
              "      <td>0.292795</td>\n",
              "      <td>0.394347</td>\n",
              "      <td>0.340689</td>\n",
              "      <td>0.365051</td>\n",
              "      <td>0.446110</td>\n",
              "      <td>0.402498</td>\n",
              "      <td>0.263965</td>\n",
              "      <td>0.356758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1281 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a93e52f1-e63f-4a17-a9ff-acce52c5a948')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a93e52f1-e63f-4a17-a9ff-acce52c5a948 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a93e52f1-e63f-4a17-a9ff-acce52c5a948');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FdRf7Nme0HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd49012b-f286-4918-e30c-a5153fb83de5"
      },
      "source": [
        "X = cov_feature_vector.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"]).copy()\n",
        "y = cov_feature_vector[\"class\"].copy()\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "res_all = [[],[],[],[],[]]\n",
        "auc_plots = []\n",
        "y_pred_proba_all = 0 ; c = 0 ; cv_count = 10\n",
        "sae_weights = sae_model(xt = X_all, xv = X.astype(float), EPOCHS = 100, af = \"relu\", lr=0.01, num_layers = 2, hl_node= 256, BATCH_SIZE=8, opt = \"adam\", do = 0.5)\n",
        "#Initiate Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=cv_count, random_state=42,shuffle=True)\n",
        "for train_ind, test_ind in cv.split(X,y):\n",
        "  #Train the model\n",
        "  X_train,y_train = X.iloc[train_ind,:],y[train_ind]\n",
        "  X_test,y_test = X.iloc[test_ind,:],y[test_ind]\n",
        "\n",
        "  #Data splitting, labelling, and normalizing\n",
        "  le = LabelEncoder()\n",
        "  y = le.fit_transform(y)\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train_mm = pd.DataFrame(data = scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "  X_test_mm = pd.DataFrame(data = scaler.transform(X_test), columns = X_test.columns)\n",
        "\n",
        "  #Fitting model\n",
        "  sae_dnn =dnn_model(xt = X_train_mm, EPOCHS = 100, af = \"relu\", lr=0.01, num_layers = 2, hl_node= 256, BATCH_SIZE=8, opt = \"adam\", do = 0.5)\n",
        "  # es = EarlyStopping(monitor='val_loss', patience=25)\n",
        "  sae_dnn.fit(X_train_mm,y_train,epochs=100,batch_size=32,verbose = False)\n",
        "\n",
        "  #Predict\n",
        "  y_pred_proba = sae_dnn.predict(X_test_mm)\n",
        "  y_pred = [1 if elem >= 0.5 else 0 for elem in y_pred_proba]\n",
        "  # y_pred_proba_all += y_pred_proba\n",
        "\n",
        "  #Calculate metrics\n",
        "  accu = accuracy_score(y_test, y_pred)\n",
        "  auc = roc_auc_score(y_test, y_pred_proba)\n",
        "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_pred, average='binary',pos_label=1)\n",
        "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_pred, average='binary',pos_label=0)\n",
        "\n",
        "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
        "  fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
        "  auc_plots.append([fpr,tpr,auc])\n",
        "  # #Show metrics\n",
        "  # print(\"CV : {}\".format(c+1));c+=1\n",
        "  # print(\"Accuracy  : {:.3f}\".format(accu))\n",
        "  # print(\"Recall    : {:.3f}\".format(recall_score))\n",
        "  # print(\"Precision : {:.3f}\".format(precision_score))\n",
        "  # print(\"ROC-AUC   : {:.3f}\".format(auc))\n",
        "  # print(\"F1_Score  : {:.3f}\".format(f1_score))\n",
        "  # print(confusion_matrix(y_test,y_pred))\n",
        "  # print(\"===================================\")\n",
        "  # print(\"===================================\")\n",
        "\n",
        "#Average and Stdv of k-fold CV\n",
        "print('Average Result of {} CV'.format(cv_count))\n",
        "print('Accuracy    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
        "print('Recall      : {0:.5f}±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
        "print('Precision   : {0:.5f}±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
        "print('ROC-AUC     : {0:.5f}±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
        "print('F1 Score    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
        "print('===================================')\n",
        "\n",
        "#Save CV result and choose auc plot with highest score\n",
        "best_auc_dc = auc_plots[np.array(res_all[3]).argmax()]\n",
        "res_all_dc = res_all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250/1250 [==============================] - 2s 1ms/step\n",
            "134/134 [==============================] - 0s 1ms/step\n",
            "1250/1250 [==============================] - 2s 1ms/step\n",
            "134/134 [==============================] - 0s 1ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Average Result of 10 CV\n",
            "Accuracy    : 0.92627±0.012\n",
            "Recall      : 0.77392±0.039\n",
            "Precision   : 0.78496±0.053\n",
            "ROC-AUC     : 0.95469±0.009\n",
            "F1 Score    : 0.77817±0.034\n",
            "===================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Average Result of 10 CV\n",
        "Accuracy    : 0.92674±0.013\n",
        "Recall      : 0.75855±0.042\n",
        "Precision   : 0.79645±0.054\n",
        "ROC-AUC     : 0.95293±0.012\n",
        "F1 Score    : 0.77568±0.036"
      ],
      "metadata": {
        "id": "wMXjE8M4_Pca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sae_dnn.save('./My Drive/Skripsi/Data/coronadata/model_sae_dnn')"
      ],
      "metadata": {
        "id": "bE9E2GGpwN4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "file = open('./My Drive/Skripsi/Data/coronadata/sae_dnn.pkl','wb')\n",
        "pickle.dump(sae_dnn,file)"
      ],
      "metadata": {
        "id": "VDusq6pbu8aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qy31rxv_5Eqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go2LnPUw2h4D"
      },
      "source": [
        "Average Result of 10 CV\n",
        "Accuracy    : 0.93984±0.009\n",
        "Recall      : 0.83016±0.043\n",
        "Precision   : 0.81733±0.050\n",
        "ROC-AUC     : 0.96979±0.009\n",
        "F1 Score    : 0.82162±0.022"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7jLnPoa5gN2"
      },
      "source": [
        "X = cov_feature_vector.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"]).copy()\n",
        "y = cov_feature_vector[\"class\"].copy()\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "res_all = [[],[],[],[],[]]\n",
        "auc_plots = []\n",
        "y_pred_proba_all = 0 ; c = 0 ; cv_count = 10\n",
        "# sae_weights = sae_model(xt = X_all, xv = X.astype(float), EPOCHS = 100, af = \"relu\", lr=0.01, num_layers = 2, hl_node= 300, BATCH_SIZE=8, opt = \"adam\", do = 0.5)\n",
        "#Initiate Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=cv_count, random_state=42,shuffle=True)\n",
        "for train_ind, test_ind in cv.split(X,y):\n",
        "  #Train the model\n",
        "  X_train,y_train = X.iloc[train_ind,:],y[train_ind]\n",
        "  X_test,y_test = X.iloc[test_ind,:],y[test_ind]\n",
        "\n",
        "  #Data splitting, labelling, and normalizing\n",
        "  le = LabelEncoder()\n",
        "  y = le.fit_transform(y)\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train_mm = pd.DataFrame(data = scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "  X_test_mm = pd.DataFrame(data = scaler.transform(X_test), columns = X_test.columns)\n",
        "\n",
        "  #Fitting model\n",
        "  sae_dnn =dnn_model(xt = X_train_mm, EPOCHS = 100, af = \"relu\", lr=0.01, num_layers = 2, hl_node= 300, BATCH_SIZE=8, opt = \"adam\", do = 0.5)\n",
        "  # es = EarlyStopping(monitor='val_loss', patience=25)\n",
        "  sae_dnn.fit(X_train_mm,y_train,epochs=100,batch_size=32,verbose = False)\n",
        "\n",
        "  #Predict\n",
        "  y_pred_proba = sae_dnn.predict(X_test_mm)\n",
        "  y_pred = [1 if elem >= 0.5 else 0 for elem in y_pred_proba]\n",
        "  # y_pred_proba_all += y_pred_proba\n",
        "\n",
        "  #Calculate metrics\n",
        "  accu = accuracy_score(y_test, y_pred)\n",
        "  auc = roc_auc_score(y_test, y_pred_proba)\n",
        "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_pred, average='binary',pos_label=1)\n",
        "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_pred, average='binary',pos_label=0)\n",
        "\n",
        "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
        "  fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
        "  auc_plots.append([fpr,tpr,auc])\n",
        "  # #Show metrics\n",
        "  # print(\"CV : {}\".format(c+1));c+=1\n",
        "  # print(\"Accuracy  : {:.3f}\".format(accu))\n",
        "  # print(\"Recall    : {:.3f}\".format(recall_score))\n",
        "  # print(\"Precision : {:.3f}\".format(precision_score))\n",
        "  # print(\"ROC-AUC   : {:.3f}\".format(auc))\n",
        "  # print(\"F1_Score  : {:.3f}\".format(f1_score))\n",
        "  # print(confusion_matrix(y_test,y_pred))\n",
        "  # print(\"===================================\")\n",
        "  # print(\"===================================\")\n",
        "\n",
        "#Average and Stdv of k-fold CV\n",
        "print('Average Result of {} CV'.format(cv_count))\n",
        "print('Accuracy    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
        "print('Recall      : {0:.5f}±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
        "print('Precision   : {0:.5f}±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
        "print('ROC-AUC     : {0:.5f}±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
        "print('F1 Score    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
        "print('===================================')\n",
        "\n",
        "#Choose auc plot with highest score\n",
        "best_auc_dnn = auc_plots[np.array(res_all[3]).argmax()]\n",
        "res_all_dnn = res_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG1T1yvvGMUZ"
      },
      "source": [
        "Average Result of 10 CV\n",
        "Accuracy    : 0.93844±0.014\n",
        "Recall      : 0.83009±0.054\n",
        "Precision   : 0.80829±0.049\n",
        "ROC-AUC     : 0.97021±0.008\n",
        "F1 Score    : 0.81791±0.041\n",
        "===================================\n",
        "\n",
        "Average Result of 10 CV\n",
        "Accuracy    : 0.93984±0.009\n",
        "Recall      : 0.83016±0.043\n",
        "Precision   : 0.81733±0.050\n",
        "ROC-AUC     : 0.96979±0.009\n",
        "F1 Score    : 0.82162±0.022\n",
        "\n",
        "Average Result of 10 CV #Without SAE\n",
        "Accuracy    : 0.93727±0.010\n",
        "Recall      : 0.83856±0.051\n",
        "Precision   : 0.79878±0.041\n",
        "ROC-AUC     : 0.96783±0.009\n",
        "F1 Score    : 0.81656±0.028\n",
        "==================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC2GkU0-TRY8"
      },
      "source": [
        "X = cov_feature_vector.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"]).copy()\n",
        "y = cov_feature_vector[\"class\"].copy()\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "res_all = [[],[],[],[],[]]\n",
        "auc_plots = []\n",
        "y_pred_proba_all = 0 ; c = 0 ; cv_count = 10\n",
        "sae_weights = sae_model(xt = X_all, xv = X.astype(float), EPOCHS = 100, af = \"relu\", lr=0.01, num_layers = 2, hl_node= 300, BATCH_SIZE=32, opt = \"adam\", do = 0.5)\n",
        "#Initiate Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=cv_count, random_state=42,shuffle=True)\n",
        "for train_ind, test_ind in cv.split(X,y):\n",
        "  #Train the model\n",
        "  X_train,y_train = X.iloc[train_ind,:],y[train_ind]\n",
        "  X_test,y_test = X.iloc[test_ind,:],y[test_ind]\n",
        "\n",
        "  #Data splitting, labelling, and normalizing\n",
        "  le = LabelEncoder()\n",
        "  y = le.fit_transform(y)\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train_mm = pd.DataFrame(data = scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "  X_test_mm = pd.DataFrame(data = scaler.transform(X_test), columns = X_test.columns)\n",
        "\n",
        "  #Fitting model\n",
        "  sae_dnn =dnn_model(xt = X_train_mm, EPOCHS = 100, af = \"relu\", lr=0.01, num_layers = 2, hl_node= 300, BATCH_SIZE=32, opt = \"adam\", do = 0.5)\n",
        "  # es = EarlyStopping(monitor='val_loss', patience=25)\n",
        "  sae_dnn.fit(X_train_mm,y_train,epochs=100,batch_size=32,verbose = False)\n",
        "\n",
        "  #Predict\n",
        "  y_pred_proba = sae_dnn.predict(X_test_mm)\n",
        "  y_pred = [1 if elem >= 0.5 else 0 for elem in y_pred_proba]\n",
        "  # y_pred_proba_all += y_pred_proba\n",
        "\n",
        "  #Calculate metrics\n",
        "  accu = accuracy_score(y_test, y_pred)\n",
        "  auc = roc_auc_score(y_test, y_pred_proba)\n",
        "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_pred, average='binary',pos_label=1)\n",
        "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_pred, average='binary',pos_label=0)\n",
        "\n",
        "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
        "  fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
        "  auc_plots.append([fpr,tpr,auc])\n",
        "  #Show metrics\n",
        "  print(\"CV : {}\".format(c+1));c+=1\n",
        "  print(\"Accuracy  : {:.3f}\".format(accu))\n",
        "  print(\"Recall    : {:.3f}\".format(recall_score))\n",
        "  print(\"Precision : {:.3f}\".format(precision_score))\n",
        "  print(\"ROC-AUC   : {:.3f}\".format(auc))\n",
        "  print(\"F1_Score  : {:.3f}\".format(f1_score))\n",
        "  print(confusion_matrix(y_test,y_pred))\n",
        "  print(\"===================================\")\n",
        "  print(\"===================================\")\n",
        "\n",
        "#Average and Stdv of k-fold CV\n",
        "print('Average Result of {} CV'.format(cv_count))\n",
        "print('Accuracy    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
        "print('Recall      : {0:.5f}±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
        "print('Precision   : {0:.5f}±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
        "print('ROC-AUC     : {0:.5f}±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
        "print('F1 Score    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
        "print('===================================')\n",
        "\n",
        "#Save CV result and choose auc plot with highest score\n",
        "best_auc_dc = auc_plots[np.array(res_all[3]).argmax()]\n",
        "res_all_dc = res_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS9KEDpLDui-"
      },
      "source": [
        "X = cov_feature_vector.drop(columns = [\"Protein\", \"CID_senyawa\", \"class\"]).copy()\n",
        "y = cov_feature_vector[\"class\"].copy()\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "res_all = [[],[],[],[],[]]\n",
        "auc_plots = []\n",
        "y_pred_proba_all = 0 ; c = 0 ; cv_count = 10\n",
        "sae_weights = sae_model(xt = X_all, xv = X.astype(float), EPOCHS = 100, af = \"relu\", lr=0.01, num_layers = 2, hl_node= 300, BATCH_SIZE=32, opt = \"adam\", do = 0.5)\n",
        "#Initiate Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=cv_count, random_state=42,shuffle=True)\n",
        "for train_ind, test_ind in cv.split(X,y):\n",
        "  #Train the model\n",
        "  X_train,y_train = X.iloc[train_ind,:],y[train_ind]\n",
        "  X_test,y_test = X.iloc[test_ind,:],y[test_ind]\n",
        "\n",
        "  #Data splitting, labelling, and normalizing\n",
        "  le = LabelEncoder()\n",
        "  y = le.fit_transform(y)\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train_mm = pd.DataFrame(data = scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "  X_test_mm = pd.DataFrame(data = scaler.transform(X_test), columns = X_test.columns)\n",
        "\n",
        "  #Fitting model\n",
        "  sae_dnn =dnn_model(xt = X_train_mm, EPOCHS = 100, af = \"relu\", lr=0.01, num_layers = 2, hl_node= 300, BATCH_SIZE=32, opt = \"adam\", do = 0.5)\n",
        "  # es = EarlyStopping(monitor='val_loss', patience=25)\n",
        "  sae_dnn.fit(X_train_mm,y_train,epochs=100,batch_size=32,verbose = False)\n",
        "\n",
        "  #Predict\n",
        "  y_pred_proba = sae_dnn.predict(X_test_mm)\n",
        "  y_pred = [1 if elem >= 0.5 else 0 for elem in y_pred_proba]\n",
        "  # y_pred_proba_all += y_pred_proba\n",
        "\n",
        "  #Calculate metrics\n",
        "  accu = accuracy_score(y_test, y_pred)\n",
        "  auc = roc_auc_score(y_test, y_pred_proba)\n",
        "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_pred, average='binary',pos_label=1)\n",
        "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_pred, average='binary',pos_label=0)\n",
        "\n",
        "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
        "  fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
        "  auc_plots.append([fpr,tpr,auc])\n",
        "  #Show metrics\n",
        "  print(\"CV : {}\".format(c+1));c+=1\n",
        "  print(\"Accuracy  : {:.3f}\".format(accu))\n",
        "  print(\"Recall    : {:.3f}\".format(recall_score))\n",
        "  print(\"Precision : {:.3f}\".format(precision_score))\n",
        "  print(\"ROC-AUC   : {:.3f}\".format(auc))\n",
        "  print(\"F1_Score  : {:.3f}\".format(f1_score))\n",
        "  print(confusion_matrix(y_test,y_pred))\n",
        "  print(\"===================================\")\n",
        "  print(\"===================================\")\n",
        "\n",
        "#Average and Stdv of k-fold CV\n",
        "print('Average Result of {} CV'.format(cv_count))\n",
        "print('Accuracy    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
        "print('Recall      : {0:.5f}±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
        "print('Precision   : {0:.5f}±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
        "print('ROC-AUC     : {0:.5f}±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
        "print('F1 Score    : {0:.5f}±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
        "print('===================================')\n",
        "\n",
        "#Save CV result and choose auc plot with highest score\n",
        "best_auc_dc = auc_plots[np.array(res_all[3]).argmax()]\n",
        "res_all_dc = res_all"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}